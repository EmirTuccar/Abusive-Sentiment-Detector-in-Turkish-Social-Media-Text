{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emiro\\AppData\\Local\\Temp\\ipykernel_18360\\3496853386.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier ,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load the data (replace 'path_to_file.xlsx' with your actual file path)\n",
    "df1 = pd.read_excel('HATC.xlsx', engine='openpyxl')\n",
    "#df2 = pd.read_excel('HATC.xlsx', engine='openpyxl')\n",
    "\n",
    "#combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "# Preprocess the data (this is an example, adjust according to your data)\n",
    "# Assume df has two columns: 'text' for the message and 'label' for the sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "#X2 = df2['message']\n",
    "#y2 = df2['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "#Xc = combined_df['message']\n",
    "#yc = combined_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the label types (they should be the same, e.g., integers)\n",
    "#print(df1['label'].dtype)\n",
    "#print(df2['label'].dtype)\n",
    "#print(combined_df['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the 'label' column of both datasets\n",
    "#print(df1['label'].unique())\n",
    "#print(df2['label'].unique())\n",
    "\n",
    "# Remove or replace non-numeric labels\n",
    "# For example, to remove rows with non-numeric labels in df1\n",
    "#df1 = df1[pd.to_numeric(df1['label'], errors='coerce').notnull()]\n",
    "#df1['label'] = df1['label'].astype(int)\n",
    "\n",
    "# Repeat for df2 if necessary\n",
    "#df2 = df2[pd.to_numeric(df2['label'], errors='coerce').notnull()]\n",
    "#df2['label'] = df2['label'].astype(int)\n",
    "\n",
    "# Concatenate the cleaned datasets\n",
    "#combined_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the labels are not integers, convert them\n",
    "\n",
    "#df2['label'] = df2['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "X1 = df1['message']\n",
    "y1 = df1['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in your target variable\n",
    "#print(\"NaNs in y_train:\", y_train.isnull().sum())\n",
    "\n",
    "# Remove rows with NaN labels from the training data\n",
    "#X_train = X_train[y_train.notnull()]\n",
    "#y_train = y_train[y_train.notnull()]\n",
    "\n",
    "# Continue with vectorization and model training...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.98      0.94      3961\n",
      "           1       0.95      0.84      0.89      2181\n",
      "           2       0.89      0.54      0.68       248\n",
      "\n",
      "    accuracy                           0.92      6390\n",
      "   macro avg       0.91      0.79      0.83      6390\n",
      "weighted avg       0.92      0.92      0.91      6390\n",
      "\n",
      "Accuracy: 0.9153364632237871\n"
     ]
    }
   ],
   "source": [
    "# Train an SVM classifier\n",
    "svm_classifier = SVC(probability=True, kernel='linear')\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "predictions = svm_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91      3961\n",
      "           1       0.91      0.75      0.82      2181\n",
      "           2       1.00      0.06      0.11       248\n",
      "\n",
      "    accuracy                           0.87      6390\n",
      "   macro avg       0.92      0.60      0.61      6390\n",
      "weighted avg       0.88      0.87      0.85      6390\n",
      "\n",
      "Accuracy: 0.8693270735524257\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "predictions = classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.91      3961\n",
      "           1       0.86      0.82      0.84      2181\n",
      "           2       0.65      0.58      0.62       248\n",
      "\n",
      "    accuracy                           0.88      6390\n",
      "   macro avg       0.80      0.78      0.79      6390\n",
      "weighted avg       0.88      0.88      0.88      6390\n",
      "\n",
      "Accuracy: 0.8784037558685446\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "decision_tree.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predictions = decision_tree.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93      3961\n",
      "           1       0.91      0.83      0.87      2181\n",
      "           2       0.89      0.44      0.59       248\n",
      "\n",
      "    accuracy                           0.90      6390\n",
      "   macro avg       0.90      0.74      0.80      6390\n",
      "weighted avg       0.90      0.90      0.90      6390\n",
      "\n",
      "Accuracy: 0.8996870109546166\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate the classifier\n",
    "predictions = rf_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.93      3961\n",
      "           1       0.93      0.80      0.86      2181\n",
      "           2       0.92      0.36      0.52       248\n",
      "\n",
      "    accuracy                           0.89      6390\n",
      "   macro avg       0.91      0.71      0.77      6390\n",
      "weighted avg       0.90      0.89      0.89      6390\n",
      "\n",
      "Accuracy: 0.8935837245696401\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "logreg_classifier = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logreg_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = logreg_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.98      0.83      3961\n",
      "           1       0.91      0.34      0.50      2181\n",
      "           2       0.77      0.35      0.48       248\n",
      "\n",
      "    accuracy                           0.74      6390\n",
      "   macro avg       0.80      0.56      0.60      6390\n",
      "weighted avg       0.78      0.74      0.70      6390\n",
      "\n",
      "Accuracy: 0.7389671361502348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the KNN model\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Train the model\n",
    "knn_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = knn_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89      3961\n",
      "           1       0.96      0.62      0.75      2181\n",
      "           2       0.86      0.46      0.60       248\n",
      "\n",
      "    accuracy                           0.84      6390\n",
      "   macro avg       0.88      0.69      0.75      6390\n",
      "weighted avg       0.86      0.84      0.83      6390\n",
      "\n",
      "Accuracy: 0.8431924882629108\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Gradient Boosting Classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Train the model\n",
    "gb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = gb_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91      3961\n",
      "           1       0.96      0.73      0.83      2181\n",
      "           2       0.84      0.48      0.61       248\n",
      "\n",
      "    accuracy                           0.88      6390\n",
      "   macro avg       0.88      0.73      0.78      6390\n",
      "weighted avg       0.89      0.88      0.87      6390\n",
      "\n",
      "Accuracy: 0.8785602503912363\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBClassifier model\n",
    "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Train the model\n",
    "xgb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "predictions = xgb_classifier.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95      3961\n",
      "           1       0.93      0.86      0.89      2181\n",
      "           2       0.87      0.65      0.74       248\n",
      "\n",
      "    accuracy                           0.92      6390\n",
      "   macro avg       0.91      0.83      0.86      6390\n",
      "weighted avg       0.92      0.92      0.92      6390\n",
      "\n",
      "Accuracy: 0.9215962441314554\n"
     ]
    }
   ],
   "source": [
    "# Create the ensemble model\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier), \n",
    "    ('svm', svm_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the ensemble\n",
    "ensemble.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict with the ensemble\n",
    "predictions = ensemble.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the ensemble\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the ensemble model with voting='soft' for averaged probabilities\n",
    "ensemble2 = VotingClassifier(estimators=[\n",
    "    ('svm', svm_classifier),\n",
    "    ('logreg', logreg_classifier),\n",
    "    ('rf', rf_classifier),\n",
    "    ('xg', xgb_classifier)],\n",
    "    voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble2.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate the ensemble model\n",
    "predictions = ensemble2.predict(X_test_tfidf)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(decision_tree, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_tree_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Ensemble Model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\u001b[43mensemble\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# SVM Classifier\u001b[39;00m\n\u001b[0;32m     11\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(svm_classifier, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm_classifier.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "joblib.dump(rf_classifier, 'rf_classifier.joblib')\n",
    "\n",
    "# Decision Tree Classifier\n",
    "joblib.dump(decision_tree, 'decision_tree_model.joblib')\n",
    "\n",
    "# Ensemble Model\n",
    "joblib.dump(ensemble, 'ensemble_model.joblib')\n",
    "\n",
    "# SVM Classifier\n",
    "joblib.dump(svm_classifier, 'svm_classifier.joblib')\n",
    "\n",
    "# Logistic Regression Classifier\n",
    "joblib.dump(logreg_classifier, 'logreg_classifier.joblib')\n",
    "\n",
    "# K-Nearest Neighbors Classifier\n",
    "joblib.dump(knn_classifier, 'knn_classifier.joblib')\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "joblib.dump(gb_classifier, 'gb_classifier.joblib')\n",
    "\n",
    "# XGBoost Classifier\n",
    "joblib.dump(xgb_classifier, 'xgb_classifier.joblib')\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "joblib.dump(classifier, 'nb_classifier.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment of new text\n",
    "def detect_abusive_sentiment(text, vectorizer_path, classifier_path):\n",
    "    # Load the saved vectorizer and classifier\n",
    "    loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "    loaded_classifier = joblib.load(classifier_path)\n",
    "\n",
    "    # Vectorize the text\n",
    "    text_vector = loaded_vectorizer.transform([text])\n",
    "\n",
    "    # Predict and return the sentiment\n",
    "    sentiment = loaded_classifier.predict(text_vector)\n",
    "    return sentiment[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "new_text = \"lan salak\"\n",
    "sentiment = detect_abusive_sentiment(new_text, 'vectorizer.joblib', 'classifier.joblib')\n",
    "print(f\"The sentiment level of the input text is: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
